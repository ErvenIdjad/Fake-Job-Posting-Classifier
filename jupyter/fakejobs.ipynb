{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91186fcc",
   "metadata": {},
   "source": [
    "Brief Description of dataset:\n",
    "\n",
    "This dataset comprises 18K job descriptions, about 800 of which are fraudulent. \n",
    "The data includes both textual and meta-information about the jobs.\n",
    "\n",
    "Objective of the project:\n",
    "Predict which job descriptions are fraudulent or authentic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f683d3",
   "metadata": {},
   "source": [
    "Metadata\n",
    "\n",
    "Name of dataset: Real or Fake Jobs\n",
    "\n",
    "Collaborators: Shivam Bansal (Owner)\n",
    "\n",
    "GEOSPATIAL COVERAGE: Worldwide\n",
    "\n",
    "Tags: education, classification, data visualization, data analytics, jobs and career, employment\n",
    "\n",
    "Modification Date: Updated 2 months ago\n",
    "\n",
    "SOURCES: http://emscad.samos.aegean.gr/\n",
    "\n",
    "License: https://creativecommons.org/publicdomain/zero/1.0/\n",
    "\n",
    "Expected Update Frequency: Never"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57874a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix, classification_report, confusion_matrix\n",
    "from wordcloud import WordCloud\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cbe676",
   "metadata": {},
   "source": [
    "# Reading dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ce7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('fake_job_postings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first 5 records\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['employment_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a6128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f3240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting unnecessary columns\n",
    "# axis =1 specifies that the values are column value and inplace=true to make these changes permanent (ie. make these dropes of columns permanent in the data set)\n",
    "# We have droped salary range because 70% approx null value\n",
    "# also job_id and other irrelvent columns because they does not have any logical meaning\n",
    "data.drop(['job_id', 'salary_range', 'telecommuting', 'has_company_logo', 'has_questions'], \n",
    "        axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb57810",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c132c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing values with blank\n",
    "data.fillna(' ', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a49ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create independent and Dependent Features\n",
    "\n",
    "#converting column names into list\n",
    "columns = data.columns.tolist()\n",
    "# Filter the columns to remove data we do not want \n",
    "columns = [c for c in columns if c not in [\"fraudulent\"]]\n",
    "# Store the variable we are predicting \n",
    "target = \"fraudulent\"\n",
    "# Define a random state \n",
    "state = np.random.RandomState(42)\n",
    "X = data[columns]\n",
    "Y = data[\"fraudulent\"]\n",
    "X_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))\n",
    "# Print the shapes of X & Y\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "from imblearn.under_sampling import RandomUnderSampler  \n",
    "\n",
    "under_sampler = RandomUnderSampler()\n",
    "X_res, y_res = under_sampler.fit_resample(X, Y)\n",
    "\n",
    "df1 = pd.DataFrame(X_res)\n",
    "  \n",
    "df2 = pd.DataFrame(y_res)\n",
    "  \n",
    "# the default behaviour is join='outer'\n",
    "# inner join\n",
    "  \n",
    "result = pd.concat([df1, df2], axis=1, join='inner')\n",
    "display(result)\n",
    "data=result;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f04e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler  \n",
    "\n",
    "under_sampler = RandomUnderSampler()\n",
    "X_res, y_res = under_sampler.fit_resample(X, Y)\n",
    "\n",
    "df1 = pd.DataFrame(X_res)\n",
    "  \n",
    "df2 = pd.DataFrame(y_res)\n",
    "  \n",
    "# the default behaviour is join='outer'\n",
    "# inner join\n",
    "  \n",
    "result = pd.concat([df1, df2], axis=1, join='inner')\n",
    "display(result)\n",
    "data=result;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec79b902",
   "metadata": {},
   "source": [
    "# Explaratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#39 Checking for distribution of percentages belonging to real class and fraud class\n",
    "# 1 = Fake post, 2 = real post\n",
    "\n",
    "labels = 'Fake', 'Real'\n",
    "sizes = [data.fraudulent[data['fraudulent']== 1].count(), data.fraudulent[data['fraudulent']== 0].count()]\n",
    "explode = (0, 0.1) \n",
    "fig1, ax1 = plt.subplots(figsize=(8, 6)) #size of the pie chart\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.2f%%',\n",
    "        shadow=True, startangle=120) #autopct %1.2f%% for 2 digit precision\n",
    "ax1.axis('equal')\n",
    "plt.title(\"Proportion of Fraudulent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bab920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing which country is posting most of the jobs\n",
    "\n",
    "def split(location):\n",
    "    l = location.split(',')\n",
    "    return l[0]\n",
    "\n",
    "data['country'] = data.location.apply(split)\n",
    "data['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e68ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will give unique country values\n",
    "data['country'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc892d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 country that post jobs \n",
    "data['country'].value_counts()[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35869a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary(key-value pair) with top 10 country\n",
    "\n",
    "country = dict(data.country.value_counts()[:11])\n",
    "del country[' '] #deleting country with space values\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.title('Country-wise Job Posting', size=15)\n",
    "plt.bar(country.keys(), country.values()) #(xaxis,yaxis)\n",
    "plt.ylabel('No. of jobs', size=10)\n",
    "plt.xlabel('Countries', size=10)\n",
    "country.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1316363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing jobs based on experience\n",
    "\n",
    "experience = dict(data.required_experience.value_counts())\n",
    "del experience[' ']\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.bar(experience.keys(), experience.values())\n",
    "plt.title('No. of Jobs with Experience')\n",
    "plt.xlabel('Experience', size=10)\n",
    "plt.ylabel('No. of jobs', size=10)\n",
    "plt.xticks(rotation=35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequent jobs\n",
    "print(data.title.value_counts()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92fa9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for most fake jobs based on title\n",
    "print(data[data.fraudulent==1].title.value_counts()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be785d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808560af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For textual type data we will try to create word cloud \n",
    "# but before that we will try to create text combining all the data present in\n",
    "# our database.\n",
    "data['text'] = data['title']+' '+data['location']+' '+data['company_profile']+' '+data['description']+' '+data['requirements']+' '+data['benefits']+' '+data['industry']+' '+data['function']+' '+data['country']+' '+data['employment_type']\n",
    "\n",
    "del data['title']\n",
    "del data['location']\n",
    "del data['department']\n",
    "del data['company_profile']\n",
    "del data['description']\n",
    "del data['requirements']\n",
    "del data['benefits']\n",
    "del data['required_experience']\n",
    "del data['required_education']\n",
    "del data['industry']\n",
    "del data['function']\n",
    "del data['country']\n",
    "del data['employment_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4de241",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b097e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# visualizing all the words in our data using the wordcloud plot\n",
    "all_words = ''.join([text for text in data[\"text\"]])\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 500, random_state=21, max_font_size=120).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b650c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing common words in real job posting\n",
    "\n",
    "real_post = ''.join([text for text in data[\"text\"][data['fraudulent']==0]])\n",
    "wordcloud = WordCloud(width = 800, height = 500, random_state=21, max_font_size=120).generate(real_post)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6adb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing common words in real job posting\n",
    "\n",
    "fraud_post = ''.join([text for text in data[\"text\"][data['fraudulent'] == 1]])\n",
    "wordcloud = WordCloud(width = 800, height = 500, random_state=21, max_font_size=120).generate(fraud_post)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30cbb0d",
   "metadata": {},
   "source": [
    "### Data Preapration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK :: Natural Language Toolkit\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dabc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words(\"english\"))\n",
    "\n",
    "#loading the stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "#converting all the text to lower case\n",
    "data['text'] = data['text'].apply(lambda x:x.lower())\n",
    "\n",
    "#removing the stop words from the corpus\n",
    "data['text'] = data['text'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12101b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting dataset in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.text, data.fraudulent, test_size=0.3)\n",
    "\n",
    "# what does X-train and y_train contain\n",
    "print(y_train)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b654ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Converting the data into vector format\n",
    "\n",
    "#  instantiate the vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "# fit\n",
    "vect.fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the document-term matrix\n",
    "X_train_dtm\n",
    "\n",
    "#how X_train_dtm is looking\n",
    "print(X_train_dtm)\n",
    "# This is Matrix representation,non 0 valued cells are not printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform testing data using fitted vocabulary into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0c6e0",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be452505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e711ae",
   "metadata": {},
   "source": [
    "## Logistic Reg. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "%time lr.fit(X_train_dtm, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86db095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_lr = lr.predict(X_test_dtm)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Classification Report\\n\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(\"Confusion Matrix\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea30cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test,y_pred_lr)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833ee5e",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "# we are using Multinomial Naive Bayes approach because the data here is not symmetrical.\n",
    "# generally if there are data in the form of this long text,it is advisable to \n",
    "# %time will give the time taken by the system for execution\n",
    "\n",
    "nb = MultinomialNB()\n",
    "%time nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7823e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70366fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nb = nb.predict(X_test_dtm)\n",
    "\n",
    "accuracy_score(y_test, y_pred_nb)\n",
    "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Classification Report\\n\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "print(\"Confusion Matrix\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_nb))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred_nb)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc5e63",
   "metadata": {},
   "source": [
    "## Linear SVC (Liblinear lib.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8385d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrsvc = LinearSVC()\n",
    "%time lrsvc.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_lrsvc = lrsvc.predict(X_test_dtm)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_lrsvc))\n",
    "print(\"Classification Report\\n\")\n",
    "print(classification_report(y_test, y_pred_lrsvc))\n",
    "print(\"Confusion Matrix\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_lrsvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf87eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test,y_pred_lrsvc)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc63c10",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "%time rf.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac15acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf.predict(X_test_dtm)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Classification Report\\n\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e720d6f",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cbb71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "#train the model \n",
    "# using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "%time dt.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = dt.predict(X_test_dtm)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_class))\n",
    "print(\"Classification Report\\n\")\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "print(\"Confusion Matrix\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9148f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test,y_pred_class)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ee586",
   "metadata": {},
   "source": [
    "## Building a Predictive System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20be9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting for a data set\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88553fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[184])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed83626",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=[\"internship (advertising) gr, , initiative led talented, energetic inspirational group young greek leaders called “global shapers athens hub”, umbrella world economic forum. nutshell, objective corporate-business community align global shapers order develop implement internship program, whereby companies commit taking board selected talented graduates 6-month internships – order offer work experience / skill building ultimately support development young talent enter local market. 6-month full-time paid internship position largest corporations greece. internship program includes classroom job training, team projects, networking profound business leaders greek job market, community service non-governmental organizations. information: #url_3a192fa44cc0cec563d796313a1fbbbaf5543bb685aa98e0143dc082adc1ab4f#candidates applying one job families, matched job family relevant studies/profile. bachelor's degree majors (aei, tei, college)limited working experiencefluent english working experience leading corporationsholistic classroom training personal developmenton job coaching specific projectsregular sessions senior leaders participating organizations marketing advertising advertising gr full-time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0978d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to feature vectors\n",
    "input_data_features = vect.transform(input_data)\n",
    "\n",
    "# making prediction\n",
    "\n",
    "prediction = rf.predict(input_data_features)\n",
    "print(prediction)\n",
    "\n",
    "\n",
    "if (prediction[0]==1):\n",
    "  print('Fraudulant Job')\n",
    "\n",
    "else:\n",
    "  print('Real Job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e28511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking wether predicted result was correct or not\n",
    "print(y_train[184])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698dfb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the model into pickle file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix, classification_report, confusion_matrix\n",
    "from flask import Flask, render_template, request, jsonify, flash\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "data = pd.read_csv('clean_fakejobs.csv')\n",
    "\n",
    "# Splitting dataset in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.text, data.fraudulent, test_size=0.3)\n",
    "\n",
    "# Converting the data into vector format\n",
    "#  instantiate the vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "# fit\n",
    "vect.fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# instantiate a Decision Tree Classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "clf = rf.fit(X_train_dtm, y_train)\n",
    "y_pred = clf.predict(X_test_dtm)\n",
    "\n",
    "# Save the vectorizer\n",
    "vectfile = 'vectorizer.pkl'\n",
    "pickle.dump(vect, open(vectfile, 'wb'))\n",
    "\n",
    "# Saving model to disk\n",
    "pickle.dump(clf, open('model.pkl','wb'))\n",
    "\n",
    "model = pickle.load(open('model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d093d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
